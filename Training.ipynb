{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DzcS3XPA7Yr",
        "outputId": "6f8cc4e8-332f-4aca-d019-e28e4c90b8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "checkpoints\t\t  log_dir_tensorboard  quadruped\n",
            "Firstlocomotionmodel.zip  __MACOSX\t       quadruped_rl.gif\n",
            "go2_walk_fixed.gif\t  Models\t       test.gif\n",
            "kp_100.0_kd_2.0_trot.gif  Monitor\t       unitree_a1\n",
            "kp_150.0_kd_2.0_trot.gif  Quadroped.zip        unitree_go1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from IPython.display import clear_output as clc\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Qadroped')\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A9d_4KQB-1Je"
      },
      "outputs": [],
      "source": [
        "!pip install mujoco\n",
        "!pip install Pillow\n",
        "!pip install 'shimmy>=2.0'\n",
        "!pip install stable-baselines3[extra] gymnasium\n",
        "clc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGBwUxdmCyO_",
        "outputId": "00d819e6-fdd6-4df3-a413-f3308356021d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['MUJOCO_GL'] = 'egl'\n",
        "import mujoco\n",
        "from mujoco import Renderer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML, display\n",
        "from IPython.display import Image as IPImage, display\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import torch\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback , EvalCallback\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.mujoco import MujocoEnv\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "SJ8FcERrpr4W"
      },
      "outputs": [],
      "source": [
        "DEFAULT_CAMERA_CONFIG = {\n",
        "    \"azimuth\": 90.0,\n",
        "    \"distance\": 3.0,\n",
        "    \"elevation\": -25.0,\n",
        "    \"lookat\": np.array([0., 0., 0.]),\n",
        "    \"fixedcamid\": 0,\n",
        "    \"trackbodyid\": -1,\n",
        "    \"type\": 2,\n",
        "}\n",
        "\n",
        "\n",
        "class QuadrupedEnv(MujocoEnv):\n",
        "    metadata = {'render.modes': ['human'],'render_fps': 25}\n",
        "    def __init__(self, ctrl_type=\"position\",**kwargs):\n",
        "\n",
        "        model_path = Path(\"unitree_a1/scene.xml\")\n",
        "        MujocoEnv.__init__(\n",
        "            self,\n",
        "            model_path=model_path.absolute().as_posix(),\n",
        "            frame_skip=20,  # Perform an action every 10 frames (dt(=0.002) * 50\n",
        "            observation_space=None,  # Manually set afterwards\n",
        "            default_camera_config=DEFAULT_CAMERA_CONFIG,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self.renderer = mujoco.Renderer(self.model, 360, 360)\n",
        "        self.cam = mujoco.MjvCamera()\n",
        "\n",
        "\n",
        "        # self._default_joint_position = np.array(self.model.key_ctrl[0])\n",
        "        # print(self._default_joint_position)\n",
        "        self._default_joint_position = np.array([-0.03, 0.9, -1.4,\n",
        "                                                 0.03, 0.9, -1.4,\n",
        "                                                 -0.03, 0.9, -1.4,\n",
        "                                                 0.03, 0.9, -1.4])\n",
        "        # self._default_joint_position = np.array(self.model.key_ctrl[0])\n",
        "        # Observation: base vel (3), ori (4), joint angles (12), joint velocities (12),previous_actions (12) = 43\n",
        "        obs_dim = 46\n",
        "        obs_high = np.array([np.inf]*obs_dim*2, dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(-obs_high, obs_high, dtype=np.float32)\n",
        "\n",
        "        self._previous_observation = np.zeros(obs_dim, dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.joint_limits_low = np.array([\n",
        "            -0.03, 0.25, -1.6,  # Front left leg\n",
        "            -0.03, 0.25, -1.6,  # Front right leg\n",
        "            -0.03, 0.25, -1.6,  # Rear left leg\n",
        "            -0.03, 0.25, -1.6   # Rear right leg\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        self.joint_limits_high = np.array([\n",
        "            0.03, 1.2, -1.1,   # Front left leg\n",
        "            0.03, 1.2, -1.1,   # Front right leg\n",
        "            0.03, 1.2, -1.1,   # Rear left leg\n",
        "            0.03, 1.2, -1.1    # Rear right leg\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.action_space = spaces.Box(\n",
        "            low=self.joint_limits_low,\n",
        "            high=self.joint_limits_high,\n",
        "            shape=(12,),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self._gravity_vector = np.array(self.model.opt.gravity)\n",
        "\n",
        "        self._feet_air_time = np.zeros(4)\n",
        "        self._last_contacts = np.zeros(4)\n",
        "\n",
        "        self.episode_reward_ = 0.0\n",
        "        self.tracking_lin_vel_reward_ = 0\n",
        "        self.tracking_ang_vel_reward_ = 0\n",
        "        self.helthy_reward_ = 0\n",
        "        self.feet_air_time_reward_ = 0\n",
        "\n",
        "        self.torque_cost_ = 0\n",
        "        self.action_diff_penalty_ = 0\n",
        "        self.lin_vel_z_penalty_ = 0\n",
        "        self.xy_angular_velocity_cost_ = 0\n",
        "        self.action_sym_ = 0\n",
        "        self.acceleration_cost_ = 0\n",
        "        self.orientation_penalty_ = 0\n",
        "        self.default_joint_position_cost_ = 0\n",
        "\n",
        "\n",
        "\n",
        "        self.step_counter = 0\n",
        "        self.episode_counter = 0\n",
        "        self.log_episode_count =  20\n",
        "        self.prev_x = 0.0\n",
        "        self.reached_target = False\n",
        "\n",
        "\n",
        "        self.goal_distance = 9.0\n",
        "        self.tracking_sigma = 0.25\n",
        "        self.maximum_episode_steps = 1024\n",
        "        self._max_episode_time_sec = 15.0\n",
        "        self._curriculum_base = 0.3\n",
        "\n",
        "        self.prev_action = np.zeros_like(self._default_joint_position)\n",
        "\n",
        "        self.target_lin_vel = self.set_target_velocity()\n",
        "        self.target_ang_vel = 0.0  # yaw rate in rad/s\n",
        "\n",
        "\n",
        "        self._healthy_z_range = (0.3, 0.335)\n",
        "        self._healthy_pitch_range = (-np.deg2rad(6), np.deg2rad(6))\n",
        "        self._healthy_roll_range = (-np.deg2rad(6), np.deg2rad(6))\n",
        "        self._healthy_yaw_range = (-np.deg2rad(10), np.deg2rad(10))\n",
        "\n",
        "        self._cfrc_ext_feet_indices = [4, 7, 10, 13]\n",
        "\n",
        "\n",
        "        feet_site = [\n",
        "            \"FR\",\n",
        "            \"FL\",\n",
        "            \"RR\",\n",
        "            \"RL\",\n",
        "        ]\n",
        "        self._feet_site_name_to_id = {\n",
        "            f: mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
        "            for f in feet_site\n",
        "        }\n",
        "\n",
        "        self._main_body_id = mujoco.mj_name2id(\n",
        "            self.model, mujoco.mjtObj.mjOBJ_BODY.value, \"trunk\"\n",
        "        )\n",
        "\n",
        "        self._gravity_vector = np.array(self.model.opt.gravity)\n",
        "\n",
        "\n",
        "\n",
        "        dof_position_limit_multiplier = 0.9  # The % of the range that is not penalized\n",
        "        ctrl_range_offset = (\n",
        "            0.5\n",
        "            * (1 - dof_position_limit_multiplier)\n",
        "            * (\n",
        "                self.model.actuator_ctrlrange[:, 1]\n",
        "                - self.model.actuator_ctrlrange[:, 0]\n",
        "            )\n",
        "        )\n",
        "        # First value is the root joint, so we ignore it\n",
        "        self._soft_joint_range = np.copy(self.model.actuator_ctrlrange)\n",
        "        self._soft_joint_range[:, 0] += ctrl_range_offset\n",
        "        self._soft_joint_range[:, 1] -= ctrl_range_offset\n",
        "\n",
        "        self.reward_weights={}\n",
        "        self.cost_weights={}\n",
        "\n",
        "    def set_weights(self, reward_weights, cost_weights):\n",
        "        self.reward_weights = reward_weights\n",
        "        self.cost_weights = cost_weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_target_velocity(self):\n",
        "        # V_x_rand = np.random.uniform(0.4, 0.7)\n",
        "        return np.array([0.5, 0.0])\n",
        "\n",
        "\n",
        "    def reset(self,seed=None,options=None):\n",
        "      super().reset(seed=seed)\n",
        "      self.reset_model()\n",
        "\n",
        "      self.prev_x = 0.0\n",
        "      self.episode_reward_ = 0.0\n",
        "      self.tracking_lin_vel_reward_ = 0\n",
        "      self.tracking_ang_vel_reward_ = 0\n",
        "      self.helthy_reward_ = 0\n",
        "      self.feet_air_time_reward_ = 0\n",
        "\n",
        "      self.torque_cost_ = 0\n",
        "      self.action_diff_penalty_ = 0\n",
        "      self.lin_vel_z_penalty_ = 0\n",
        "      self.xy_angular_velocity_cost_ = 0\n",
        "      self.action_sym_ = 0\n",
        "      self.acceleration_cost_ = 0\n",
        "      self.orientation_penalty_ = 0\n",
        "      self.default_joint_position_cost_ = 0\n",
        "      self.target_lin_vel = self.set_target_velocity()\n",
        "\n",
        "      current_obs = self._get_obs()\n",
        "      self._previous_observation = np.zeros_like(current_obs)\n",
        "      full_obs = np.concatenate([current_obs, self._previous_observation])\n",
        "      self._previous_observation = current_obs.copy()\n",
        "\n",
        "      return full_obs,{}\n",
        "\n",
        "    def reset_model(self):\n",
        "        self.step_counter = 0\n",
        "        self.data.qpos[0:3] = [0.0, 0.0, 0.31]\n",
        "        # self.data.qpos[3:7] = [1.0, 0.0, 0.0, 0.0]\n",
        "        # self.data.qpos[3:7] = [0.0, 1.0, 0.0, 0.0]\n",
        "\n",
        "        self.data.qpos[7:19] = self._default_joint_position.copy()\n",
        "\n",
        "        self.data.qvel[:] = 0\n",
        "        self.data.ctrl[:] = 0\n",
        "        mujoco.mj_forward(self.model, self.data)\n",
        "        self._feet_air_time = np.zeros(4)\n",
        "        self._last_contacts = np.zeros(4)\n",
        "        self.prev_action = self._default_joint_position.copy()\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "      self.step_counter += 1\n",
        "      ALPHA = 0.7\n",
        "      action_filtered = ALPHA * self.prev_action + (1 - ALPHA) * action\n",
        "      if self.step_counter < 1:\n",
        "        action_filtered = self._default_joint_position.copy()\n",
        "      self.do_simulation(action_filtered, self.frame_skip)\n",
        "\n",
        "      current_obs = self._get_obs()\n",
        "      obs = np.concatenate([current_obs, self._previous_observation])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      reward = self._compute_reward(current_obs, action)\n",
        "\n",
        "      is_healthy_ , status=  self.is_healthy\n",
        "      terminated   = not is_healthy_\n",
        "      truncated = self.step_counter >= self.maximum_episode_steps\n",
        "\n",
        "      info={'terminated': terminated,\n",
        "                'truncated:': truncated,}\n",
        "      self.prev_action = action_filtered\n",
        "      self._previous_observation = current_obs.copy()\n",
        "\n",
        "\n",
        "      if terminated or truncated:\n",
        "            self.episode_counter += 1\n",
        "            info.update({\n",
        "                \"total_reward\": self.episode_reward_,\n",
        "                \"tracking_lin_vel_reward\": self.tracking_lin_vel_reward_,\n",
        "                \"tracking_ang_vel_reward\": self.tracking_ang_vel_reward_,\n",
        "                \"healthy_reward\": self.helthy_reward_,\n",
        "                \"feet_air_time_reward\": self.feet_air_time_reward_,\n",
        "                \"torque_cost\": self.torque_cost_,\n",
        "                \"action_diff_penalty\": self.action_diff_penalty_,\n",
        "                \"lin_vel_z_penalty\": self.lin_vel_z_penalty_,\n",
        "                \"xy_angular_velocity_cost\": self.xy_angular_velocity_cost_,\n",
        "                \"action_sym\": self.action_sym_,\n",
        "                \"acceleration_cost\": self.acceleration_cost_,\n",
        "                \"orientation_penalty\": self.orientation_penalty_,\n",
        "                \"default_joint_position_cost\": self.default_joint_position_cost_,\n",
        "                \"terminated\": terminated,\n",
        "                \"truncated\": truncated,  # <-- small typo fix: removed ':' from the key\n",
        "                \"status\": status\n",
        "            })\n",
        "\n",
        "      return obs, reward, terminated, truncated,info\n",
        "\n",
        "\n",
        "    def save_animation(self, filename=\"rl_quadruped.gif\", fps=10):\n",
        "      if hasattr(self, 'frames') and self.frames:\n",
        "          print(f\"Saving {len(self.frames)} frames to {filename}\")\n",
        "          self.frames[0].save(\n",
        "              filename,\n",
        "              save_all=True,\n",
        "              append_images=self.frames[1:],\n",
        "              duration=1000 // fps,\n",
        "              loop=0\n",
        "          )\n",
        "\n",
        "          display(IPImage(filename))\n",
        "      else:\n",
        "          print(\"No frames to save.\")\n",
        "\n",
        "\n",
        "    def _get_obs(self):\n",
        "\n",
        "        base_pos = self._get_position_data()\n",
        "        base_vel = self._get_linear_velocity()\n",
        "        base_orn = self._get_orientation_data()\n",
        "        base_ang_vel = self._get_roll_pitch_yaw()\n",
        "        joint_positions = self._get_joint_data()\n",
        "        joint_velocities = self._get_joint_velocity()\n",
        "        previous_action = self.prev_action\n",
        "        reference_vel = self.target_lin_vel\n",
        "        target_lin_vel = self.target_lin_vel\n",
        "        target_ang_vel = np.array([0.0])\n",
        "\n",
        "        feet_contact_force_mag = self.feet_contact_forces\n",
        "        curr_contact = feet_contact_force_mag > 1.0\n",
        "        curr_contact = np.array(curr_contact, dtype=np.float32)\n",
        "\n",
        "        # 3 + 12 + 12 + 4 + 3 + 12\n",
        "        obs = np.concatenate((\n",
        "            base_ang_vel ,\n",
        "            joint_positions ,\n",
        "            joint_velocities,\n",
        "            curr_contact,\n",
        "            target_lin_vel,\n",
        "            target_ang_vel,\n",
        "            previous_action), dtype=np.float32)\n",
        "        return obs\n",
        "\n",
        "# ===================================== Rewards=================================\n",
        "\n",
        "    def _compute_reward(self, obs, action):\n",
        "      # Decompose observation\n",
        "      base_pos = self._get_position_data()\n",
        "      base_orn = self._get_orientation_data()\n",
        "      joint_positions = self._get_joint_data()\n",
        "      joint_velocities = self._get_joint_velocity()\n",
        "      lin_vel = self._get_linear_velocity()\n",
        "      ang_vel = self._get_angular_velocity()\n",
        "      lin_vel = np.array(lin_vel)\n",
        "      ang_vel = np.array(ang_vel)\n",
        "\n",
        "\n",
        "\n",
        "      (tracking_lin_vel_reward,\n",
        "       tracking_ang_vel_reward,\n",
        "       lin_vel_z_penalty) = self._tracking_velocity_penalty(lin_vel, ang_vel)\n",
        "\n",
        "      helthy_reward , status = self.is_healthy\n",
        "      feet_air_time_reward = self.feet_air_time_reward\n",
        "\n",
        "\n",
        "      torque_cost = self.torque_cost(joint_velocities)\n",
        "      action_diff_penalty = self._action_diff_penalty(action)\n",
        "      xy_angular_velocity_cost = self.xy_angular_velocity_cost\n",
        "      joint_limit_cost = self.joint_limit_cost\n",
        "      acceleration_cost = self.acceleration_cost\n",
        "      orientation_cost = self.non_flat_base_cost\n",
        "      default_joint_position_cost = self.default_joint_position_cost\n",
        "      action_sym = self.action_sym(action)\n",
        "\n",
        "\n",
        "\n",
        "      Positive_rewards = (tracking_lin_vel_reward *self.reward_weights[\"linear_vel_tracking\"]+\n",
        "                          tracking_ang_vel_reward * self.reward_weights[\"angular_vel_tracking\"] +\n",
        "                          helthy_reward * self.reward_weights[\"healthy\"] +\n",
        "                          feet_air_time_reward * self.reward_weights[\"feet_airtime\"])\n",
        "\n",
        "      Negative_rewards = (torque_cost * self.cost_weights[\"torque\"] +\n",
        "                          action_diff_penalty * self.cost_weights[\"action_rate\"] +\n",
        "                          lin_vel_z_penalty * self.cost_weights[\"vertical_vel\"] +\n",
        "                          xy_angular_velocity_cost * self.cost_weights[\"xy_angular_vel\"] +\n",
        "                          action_sym * self.cost_weights[\"action_sym\"] +\n",
        "                          acceleration_cost * self.cost_weights[\"joint_acceleration\"] +\n",
        "                          orientation_cost * self.cost_weights[\"orientation\"] +\n",
        "                          default_joint_position_cost * self.cost_weights[\"default_joint_position\"])\n",
        "\n",
        "\n",
        "      # reward = max(0,Positive_rewards - Negative_rewards)\n",
        "      reward= Positive_rewards - Negative_rewards\n",
        "      # reward = Positive_rewards - self.curriculum_factor * Negative_rewards\n",
        "\n",
        "\n",
        "\n",
        "      self.tracking_lin_vel_reward_ += tracking_lin_vel_reward *self.reward_weights[\"linear_vel_tracking\"]\n",
        "      self.tracking_ang_vel_reward_ += tracking_ang_vel_reward * self.reward_weights[\"angular_vel_tracking\"]\n",
        "      self.helthy_reward_ += helthy_reward * self.reward_weights[\"healthy\"]\n",
        "      self.feet_air_time_reward_ += feet_air_time_reward * self.reward_weights[\"feet_airtime\"]\n",
        "\n",
        "      self.torque_cost_ -= torque_cost * self.cost_weights[\"torque\"]\n",
        "      self.action_diff_penalty_ -= action_diff_penalty * self.cost_weights[\"action_rate\"]\n",
        "      self.lin_vel_z_penalty_ -= lin_vel_z_penalty * self.cost_weights[\"vertical_vel\"]\n",
        "      self.xy_angular_velocity_cost_ -= xy_angular_velocity_cost * self.cost_weights[\"xy_angular_vel\"]\n",
        "      self.action_sym_ -= action_sym * self.cost_weights[\"action_sym\"]\n",
        "      self.acceleration_cost_ -= acceleration_cost * self.cost_weights[\"joint_acceleration\"]\n",
        "      self.orientation_penalty_ -= orientation_cost * self.cost_weights[\"orientation\"]\n",
        "      self.default_joint_position_cost_ -= default_joint_position_cost * self.cost_weights[\"default_joint_position\"]\n",
        "\n",
        "\n",
        "\n",
        "      self.episode_reward_ += reward\n",
        "\n",
        "      return reward\n",
        "\n",
        "    def _height_penalty(self, obs):\n",
        "      z = self._get_height_data()\n",
        "      height_penalty = -((z - 0.3)**2)/ 0.05**2\n",
        "      return height_penalty\n",
        "\n",
        "    def _pose_penalty(self, obs):\n",
        "      thigh_indices = np.array([0, 3, 6, 9])\n",
        "      hip_indices = np.array([1, 4, 7, 10])\n",
        "      joints_positions = self._get_joint_data()\n",
        "      pose_error=0\n",
        "      pose_error = -np.mean(np.square((joints_positions - self.default_joint_pose)))\n",
        "      # pose_error += -np.sum(np.square(action[hip_indices] - self.default_joint_pose[hip_indices]))\n",
        "\n",
        "      return pose_error\n",
        "\n",
        "    def _action_diff_penalty(self, action):\n",
        "      if not hasattr(self, 'prev_action'):\n",
        "        self.prev_action = np.zeros_like(action)\n",
        "\n",
        "      action_diff_penalty = np.sum(np.abs(action - self.prev_action))\n",
        "      # self.prev_action = action.copy()\n",
        "      return action_diff_penalty\n",
        "\n",
        "\n",
        "\n",
        "    def _tracking_velocity_penalty(self, lin_vel, ang_vel):\n",
        "\n",
        "\n",
        "\n",
        "      # v_x = self.get_projected_vx()\n",
        "      v_x = lin_vel[0]\n",
        "\n",
        "      lin_vel_error = np.sum(np.abs(self.target_lin_vel[0] - v_x))\n",
        "\n",
        "      tracking_lin_vel_reward = np.exp(-lin_vel_error / self.tracking_sigma)\n",
        "      # tracking_lin_vel_reward = -np.sum(np.abs(self.target_lin_vel - lin_vel[:2]))\n",
        "\n",
        "      ang_vel_error = np.sum(np.square(self.target_ang_vel - ang_vel[2]))\n",
        "      tracking_ang_vel_reward = np.exp(-ang_vel_error / self.tracking_sigma)\n",
        "      # tracking_ang_vel_reward = -np.sum(np.abs(self.target_ang_vel - ang_vel[2]))\n",
        "\n",
        "      lin_vel_z_penalty = np.square((lin_vel[2]))\n",
        "\n",
        "      return tracking_lin_vel_reward, tracking_ang_vel_reward, lin_vel_z_penalty\n",
        "\n",
        "    def get_projected_vx(self):\n",
        "        roll,pitch,yaw = self._get_roll_pitch_yaw()\n",
        "        lin_vel = self._get_linear_velocity()\n",
        "        projected_vx = np.cos(yaw) * lin_vel[0] + np.sin(yaw) * lin_vel[1]\n",
        "        return projected_vx\n",
        "\n",
        "\n",
        "    @property\n",
        "    def feet_air_time_reward(self):\n",
        "        \"\"\"Award strides depending on their duration only when the feet makes contact with the ground\"\"\"\n",
        "        feet_contact_force_mag = self.feet_contact_forces\n",
        "        curr_contact = feet_contact_force_mag > 1.0\n",
        "        contact_filter = np.logical_or(curr_contact, self._last_contacts)\n",
        "        self._last_contacts = curr_contact\n",
        "\n",
        "        # if feet_air_time is > 0 (feet was in the air) and contact_filter detects a contact with the ground\n",
        "        # then it is the first contact of this stride\n",
        "        first_contact = (self._feet_air_time > 0.0) * contact_filter\n",
        "        self._feet_air_time += self.dt\n",
        "\n",
        "        # Award the feets that have just finished their stride (first step with contact)\n",
        "        air_time_reward = np.sum((self._feet_air_time) * first_contact)\n",
        "        # No award if the desired velocity is very low (i.e. robot should remain stationary and feet shouldn't move)\n",
        "        air_time_reward *= np.linalg.norm(self.target_lin_vel) > 0.1\n",
        "\n",
        "        # zero-out the air time for the feet that have just made contact (i.e. contact_filter==1)\n",
        "        self._feet_air_time *= ~contact_filter\n",
        "\n",
        "        return air_time_reward\n",
        "\n",
        "\n",
        "\n",
        "    def torque_cost(self,joint_velocities):\n",
        "        motor_torques = self.get_joint_effort()\n",
        "        # loss = np.sum(np.abs(motor_torques*joint_velocities))\n",
        "        loss = np.sum(np.abs(motor_torques))\n",
        "        return loss\n",
        "\n",
        "\n",
        "    @property\n",
        "    def xy_angular_velocity_cost(self):\n",
        "        return np.sum(np.square(self.data.qvel[5]))\n",
        "\n",
        "\n",
        "    @property\n",
        "    def joint_limit_cost(self):\n",
        "        # Penalize the robot for joints exceeding the soft control range\n",
        "        out_of_range = (self._soft_joint_range[:, 0] - self.data.qpos[7:]).clip(\n",
        "            min=0.0\n",
        "        ) + (self.data.qpos[7:] - self._soft_joint_range[:, 1]).clip(min=0.0)\n",
        "        return np.sum(out_of_range)\n",
        "\n",
        "\n",
        "    @property\n",
        "    def acceleration_cost(self):\n",
        "        return np.sum(np.square(self.data.qacc[6:]))\n",
        "\n",
        "\n",
        "    @property\n",
        "    def projected_gravity(self):\n",
        "        w, x, y, z = self.data.qpos[3:7]\n",
        "        euler_orientation = np.array(self.euler_from_quaternion(w, x, y, z))\n",
        "        projected_gravity_not_normalized = (\n",
        "            np.dot(self._gravity_vector, euler_orientation) * euler_orientation\n",
        "        )\n",
        "        if np.linalg.norm(projected_gravity_not_normalized) == 0:\n",
        "            return projected_gravity_not_normalized\n",
        "        else:\n",
        "            return projected_gravity_not_normalized / np.linalg.norm(\n",
        "                projected_gravity_not_normalized\n",
        "            )\n",
        "\n",
        "\n",
        "    # def non_flat_base_cost(self):\n",
        "    #     # Penalize the robot for not being flat on the ground\n",
        "    #     return np.sum(np.square(self.projected_gravity[:2]))\n",
        "\n",
        "\n",
        "    @property\n",
        "    def default_joint_position_cost2(self):\n",
        "        return np.sum(np.square(self.data.qpos[7:] - self._default_joint_position))\n",
        "\n",
        "\n",
        "    @property\n",
        "    def default_joint_position_cost(self):\n",
        "        joint_pos = self._get_joint_data()\n",
        "\n",
        "        soft_joint_limits_low = np.array([\n",
        "            -0.01, 0.6, -2.1,  # Front left leg\n",
        "            -0.01, 0.6, -2.1,  # Front right leg\n",
        "            -0.01, 0.6, -2.1,  # Rear left leg\n",
        "            -0.01, 0.6, -2.1   # Rear right leg\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        soft_joint_limits_high = np.array([\n",
        "            0.01, 1.1, -1.5,   # Front left leg\n",
        "            0.01, 1.1, -1.5,   # Front right leg\n",
        "            0.01, 1.1, -1.5,   # Rear left leg\n",
        "            0.01, 1.1, -1.5    # Rear right leg\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        lower_violation = np.maximum(soft_joint_limits_low - joint_pos, 0)\n",
        "        upper_violation = np.maximum(joint_pos - soft_joint_limits_high, 0)\n",
        "\n",
        "        total_violation = lower_violation + upper_violation\n",
        "\n",
        "        # Square the violations to penalize larger ones more\n",
        "        return np.sum(np.square(total_violation))\n",
        "\n",
        "\n",
        "    @property\n",
        "    def non_flat_base_cost(self):\n",
        "\n",
        "        roll, pitch, _ = self._get_roll_pitch_yaw()\n",
        "\n",
        "        return np.square(roll) + np.square(pitch)\n",
        "\n",
        "\n",
        "\n",
        "    def action_sym(self, action):\n",
        "    # Diagonal pairs expected to move in-phase (same value)\n",
        "        in_phase_pairs_thigh = [(1, 7), (4, 10)]\n",
        "\n",
        "        out_of_phase_pairs_thigh = [(1, 4), (7, 10)]\n",
        "\n",
        "        out_of_phase_pairs_calf = [(2, 5), (8, 11)]\n",
        "\n",
        "        in_phase_pairs_calf = [(2, 8), (5, 11)]\n",
        "\n",
        "\n",
        "\n",
        "        jointpositions = self._get_joint_data()  # array of normalized joint angles in [0, 1]\n",
        "\n",
        "        loss_in_thigh = sum((jointpositions[i] - jointpositions[j]) ** 2 for i, j in in_phase_pairs_thigh)\n",
        "\n",
        "        loss_out_thigh = sum((jointpositions[i] + jointpositions[j] - 1.5) ** 2 for i, j in out_of_phase_pairs_thigh)\n",
        "\n",
        "        loss_in_calf = sum((jointpositions[i] - jointpositions[j]) ** 2 for i, j in in_phase_pairs_calf)\n",
        "        loss_out_calf = sum((jointpositions[i] + jointpositions[j] + 3.6) ** 2 for i, j in out_of_phase_pairs_calf)\n",
        "\n",
        "        loss_in = loss_in_thigh + 0.1*loss_in_calf\n",
        "        loss_out = loss_out_thigh + 0.1*loss_out_calf\n",
        "        total_loss = loss_in + loss_out\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "# ======================================================================\n",
        "\n",
        "\n",
        "\n",
        "    @property\n",
        "    def is_healthy(self):\n",
        "      x, y, z = self._get_position_data()\n",
        "      roll, pitch, yaw = self._get_roll_pitch_yaw()\n",
        "\n",
        "      min_z, max_z = self._healthy_z_range\n",
        "      if not (min_z <= z <= max_z):\n",
        "          return False, f\"z out of range ({z:.3f} not in [{min_z}, {max_z}])\"\n",
        "\n",
        "      min_roll, max_roll = self._healthy_roll_range\n",
        "      if not (min_roll <= roll <= max_roll):\n",
        "          return False, f\"roll out of range ({roll:.3f} not in [{min_roll}, {max_roll}])\"\n",
        "\n",
        "      min_pitch, max_pitch = self._healthy_pitch_range\n",
        "      if not (min_pitch <= pitch <= max_pitch):\n",
        "          return False, f\"pitch out of range ({pitch:.3f} not in [{min_pitch}, {max_pitch}])\"\n",
        "\n",
        "      min_yaw, max_yaw = self._healthy_yaw_range\n",
        "      if not (min_yaw <= yaw <= max_yaw):\n",
        "          return False, f\"yaw out of range ({yaw:.3f} not in [{min_yaw}, {max_yaw}])\"\n",
        "\n",
        "      return True, \"healthy\"\n",
        "\n",
        "\n",
        "    @property\n",
        "    def feet_contact_forces(self):\n",
        "        feet_contact_forces = self.data.cfrc_ext[self._cfrc_ext_feet_indices].copy()\n",
        "        return np.linalg.norm(feet_contact_forces, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def euler_from_quaternion(w, x, y, z):\n",
        "        \"\"\"\n",
        "        Convert a quaternion into euler angles (roll, pitch, yaw)\n",
        "        roll is rotation around x in radians (counterclockwise)\n",
        "        pitch is rotation around y in radians (counterclockwise)\n",
        "        yaw is rotation around z in radians (counterclockwise)\n",
        "        \"\"\"\n",
        "        t0 = +2.0 * (w * x + y * z)\n",
        "        t1 = +1.0 - 2.0 * (x * x + y * y)\n",
        "        roll_x = np.arctan2(t0, t1)\n",
        "\n",
        "        t2 = +2.0 * (w * y - z * x)\n",
        "        t2 = +1.0 if t2 > +1.0 else t2\n",
        "        t2 = -1.0 if t2 < -1.0 else t2\n",
        "        pitch_y = np.arcsin(t2)\n",
        "\n",
        "        t3 = +2.0 * (w * z + x * y)\n",
        "        t4 = +1.0 - 2.0 * (y * y + z * z)\n",
        "        yaw_z = np.arctan2(t3, t4)\n",
        "\n",
        "        return roll_x, pitch_y, yaw_z\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "\n",
        "    @property\n",
        "    def curriculum_factor(self):\n",
        "        return self._curriculum_base**0.997\n",
        "\n",
        "\n",
        "    def _check_termination(self, obs):\n",
        "\n",
        "        base_pos = self._get_position_data()\n",
        "        base_orn = self._get_orientation_data()\n",
        "        joint_positions = self._get_joint_data()\n",
        "        joint_velocities = self._get_joint_velocity()\n",
        "\n",
        "\n",
        "        x, y, z = base_pos\n",
        "\n",
        "        # 1. Height check - robot fell over\n",
        "        if z < 0.15:\n",
        "            if self.episode_counter % self.log_episode_count == 0:\n",
        "                print(\"height fell\")\n",
        "            return True\n",
        "\n",
        "\n",
        "        if x >= self.goal_distance:\n",
        "          self.reached_target = True\n",
        "          if self.episode_counter % self.log_episode_count == 0:\n",
        "              print(\"goal reached\")\n",
        "          return True\n",
        "\n",
        "        # 2. Excessive tilt - robot is too tilted to recover\n",
        "        roll, pitch ,_= self._get_roll_pitch_yaw()\n",
        "        max_tilt = np.pi/6  # 60 degrees\n",
        "        if abs(roll) > max_tilt or abs(pitch) > max_tilt:\n",
        "            if self.episode_counter % self.log_episode_count == 0:\n",
        "                print(\"too tilted\")\n",
        "            return True\n",
        "\n",
        "        # 3. Lateral drift - robot moved too far sideways\n",
        "        max_lateral_drift = 2.0  # meters\n",
        "        if abs(y) > max_lateral_drift:\n",
        "            if self.episode_counter % self.log_episode_count == 0:\n",
        "                print(\"lateral drift\")\n",
        "            return True\n",
        "\n",
        "\n",
        "\n",
        "        return False\n",
        "    def create_animation(self, filename=\"go2_walk.gif\", fps=30):\n",
        "        if not self.simulation_data['images']:\n",
        "            print(\"No images captured.\")\n",
        "            return\n",
        "        self.simulation_data['images'][0].save(\n",
        "            filename, save_all=True, append_images=self.simulation_data['images'][1:],\n",
        "            duration=1000 // fps, loop=0\n",
        "        )\n",
        "        print(f\"Animation saved to {filename}\")\n",
        "\n",
        "\n",
        "\n",
        "    def _get_position_data(self):\n",
        "        return self.data.qpos[0:3].copy()\n",
        "\n",
        "    def _get_orientation_data(self):\n",
        "        return self.data.qpos[3:7].copy()\n",
        "\n",
        "    def _get_joint_data(self):\n",
        "        return self.data.qpos[7:19].copy()\n",
        "\n",
        "    def _get_height_data(self):\n",
        "        return self._get_position_data()[-1]\n",
        "\n",
        "\n",
        "\n",
        "    def _get_roll_pitch_yaw(self):\n",
        "        quat = self._get_orientation_data()\n",
        "        w, x, y, z = quat\n",
        "        roll = np.arctan2(2*(w*x + y*z), 1 - 2*(x*x + y*y))\n",
        "        pitch = np.arcsin(2*(w*y - z*x))\n",
        "        yaw = np.arctan2(2*(w*z + x*y), 1 - 2*(y*y + z*z))\n",
        "        return roll, pitch, yaw\n",
        "\n",
        "\n",
        "\n",
        "    def get_joint_effort(self):\n",
        "        return self.data.ctrl[:12].copy()\n",
        "\n",
        "    def _get_velocity_data(self):\n",
        "        return self.data.qvel.copy()\n",
        "\n",
        "    def _get_linear_velocity(self):\n",
        "        return self._get_velocity_data()[0:3]\n",
        "\n",
        "    def _get_angular_velocity(self):\n",
        "        return self._get_velocity_data()[3:6]\n",
        "\n",
        "    def _get_joint_velocity(self):\n",
        "        return self._get_velocity_data()[6:]\n",
        "\n",
        "\n",
        "    def _get_joint_names(self):\n",
        "\n",
        "        print(\"Joint Names and Indices:\")\n",
        "        for i in range(self.model.njnt):\n",
        "            joint_name = mujoco.mj_id2name(self.model, mujoco.mjtObj.mjOBJ_JOINT, i)\n",
        "            print(f\"  Index {i}: {joint_name}\")\n",
        "\n",
        "\n",
        "    def set_joint_positions(self, joint_angles):\n",
        "        self.data.ctrl[:12] = joint_angles\n",
        "\n",
        "\n",
        "    def capture_frame(self):\n",
        "        self.cam.lookat[:] = self._get_position_data()\n",
        "        self.renderer.update_scene(self.data,self.cam)\n",
        "        img = self.renderer.render()\n",
        "        return Image.fromarray(img)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "      if not hasattr(self, 'frames'):\n",
        "          self.frames = []\n",
        "\n",
        "      img = self.capture_frame()\n",
        "      self.frames.append(img)\n",
        "\n",
        "    def close(self):\n",
        "        return super().close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "lcB9hg572PXz"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import os\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "class RewardCSVLoggerCallback(BaseCallback):\n",
        "    def __init__(self, csv_path: str, verbose=0, log_freq=100,print_freq=100):\n",
        "        super().__init__(verbose)\n",
        "        self.csv_path = csv_path\n",
        "        self.log_freq = log_freq\n",
        "        self.print_freq = print_freq\n",
        "        self.episode_count = 0\n",
        "\n",
        "    def _on_training_start(self) -> None:\n",
        "        # Create new CSV and write header\n",
        "        with open(self.csv_path, mode='w', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow([\n",
        "                'step', 'env_id', 'total_reward',\n",
        "                'tracking_lin_vel_reward', 'tracking_ang_vel_reward', 'healthy_reward',\n",
        "                'feet_air_time_reward', 'torque_cost', 'action_diff_penalty',\n",
        "                'lin_vel_z_penalty', 'xy_angular_velocity_cost', 'action_sym',\n",
        "                'acceleration_cost', 'orientation_penalty', 'default_joint_position_cost'\n",
        "            ])\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        infos = self.locals['infos']\n",
        "        dones = self.locals['dones']\n",
        "\n",
        "\n",
        "        step_num = self.num_timesteps\n",
        "\n",
        "        rows = []\n",
        "        for env_id, (done, info) in enumerate(zip(dones,infos)):\n",
        "            if done or info['terminated']:\n",
        "                self.episode_count += 1\n",
        "\n",
        "\n",
        "                if self.episode_count % self.log_freq == 0:\n",
        "\n",
        "                      row = [\n",
        "                          step_num,\n",
        "                          env_id,\n",
        "                          info['total_reward'],\n",
        "                          info['tracking_lin_vel_reward'],\n",
        "                          info['tracking_ang_vel_reward'],\n",
        "                          info['healthy_reward'],\n",
        "                          info['feet_air_time_reward'],\n",
        "                          info['torque_cost'],\n",
        "                          info['action_diff_penalty'],\n",
        "                          info['lin_vel_z_penalty'],\n",
        "                          info['xy_angular_velocity_cost'],\n",
        "                          info['action_sym'],\n",
        "                          info['acceleration_cost'],\n",
        "                          info['orientation_penalty'],\n",
        "                          info['default_joint_position_cost'],\n",
        "                      ]\n",
        "                      rows.append(row)\n",
        "                      with open(self.csv_path, mode='a', newline='') as f:\n",
        "                          writer = csv.writer(f)\n",
        "                          writer.writerows(rows)\n",
        "\n",
        "                      if self.verbose > 0 and self.episode_count % self.print_freq == 0:\n",
        "                          clc()\n",
        "                          print(f\"status: {info['status']}\")\n",
        "                          print(f\"Episode {self.episode_count}\")\n",
        "                          print(f\"step count: {step_num}\")\n",
        "                                      #2                  #3                #4                  #5              #6                  #7                  #8                   #11                      #14\n",
        "                          print(f\"{'EpReward':>10} | {'TrackLin':>10} | {'TrackAng':>10} | {'Healthy':>10} | {'AirTime':>10} | {'Torque':>10} | {'Î”Action':>10} |  {'action_sym':>10} | {'yaw vel':>10}\")\n",
        "                          print(\n",
        "                              f\"{row[2]:10.2f} | {row[3]:10.2f} | {row[4]:10.2f} | {row[5]:10.2f} | {row[6]:10.2f} | \"\n",
        "                              f\"{row[7]:10.2f} | {row[8]:10.2f} | \"\n",
        "                              f\"{row[11]:10.2f}  | {row[10]:10.2f} |\"\n",
        "                          )\n",
        "\n",
        "\n",
        "\n",
        "        return True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls Models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DJlYQxnItvH",
        "outputId": "f2387256-e08e-49cf-862a-4645b2c5881c"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Mujoco v 3 2025-08-16_13-00-26.zip'  'Mujoco v 3 2025-08-16_21-37-28.zip'\n",
            "'Mujoco v 3 2025-08-16_14-06-17.zip'  'Mujoco v 3 2025-08-16_22-38-16.zip'\n",
            "'Mujoco v 3 2025-08-16_15-36-34.zip'  'Mujoco v 3 2025-08-17_00-20-42.zip'\n",
            "'Mujoco v 3 2025-08-16_16-12-04.zip'  'Mujoco v 3 2025-08-17_08-13-01.zip'\n",
            "'Mujoco v 3 2025-08-16_16-47-24.zip'  'Mujoco v 3 2025-08-17_10-08-34.zip'\n",
            "'Mujoco v 3 2025-08-16_19-58-21.zip'  'Mujoco v 3 2025-08-17_10-57-56.zip'\n",
            "'Mujoco v 3 2025-08-16_20-46-51.zip'  'Mujoco v 3 2025-08-17_11-43-41.zip'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "IRYwFXXXrRtM"
      },
      "outputs": [],
      "source": [
        "tensorboard_root_log_directory = \"./log_dir_tensorboard/\"\n",
        "os.makedirs(tensorboard_root_log_directory, exist_ok=True)\n",
        "TENSORBOARD_RUN_NAME = \"quadruped_ppo_training_run\"\n",
        "\n",
        "\n",
        "# env = QuadrupedEnv()\n",
        "# env = Monitor(env, monitor_log_directory)\n",
        "\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# reward_weights = {\n",
        "#     \"linear_vel_tracking\": 3,  # Was 1.0\n",
        "#     \"angular_vel_tracking\":0.1,\n",
        "#     \"healthy\": 2.5,  # was 0.05\n",
        "#     \"feet_airtime\": 15,\n",
        "# }\n",
        "# cost_weights = {\n",
        "#     \"torque\": 0.15,\n",
        "#     \"vertical_vel\": 0.0,  # Was 1.0\n",
        "#     \"xy_angular_vel\": 0.05*0,  # Was 0.05\n",
        "#     \"action_rate\": 0.5,\n",
        "#     \"action_sym\": 0.5,\n",
        "#     \"joint_velocity\": 0.01*0,\n",
        "#     \"joint_acceleration\": 2.5e-7*0,\n",
        "#     \"orientation\": 1.0*0,\n",
        "#     \"collision\": 1.0*0,\n",
        "#     \"default_joint_position\": 0.6\n",
        "# }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "reward_weights = {\n",
        "    \"linear_vel_tracking\": 3.5,  # Was 1.0\n",
        "    \"angular_vel_tracking\":0.1,\n",
        "    \"healthy\": 1,  # was 0.05\n",
        "    \"feet_airtime\": 20,\n",
        "}\n",
        "cost_weights = {\n",
        "    \"torque\": 0.2,\n",
        "    \"vertical_vel\": 0.0,  # Was 1.0\n",
        "    \"xy_angular_vel\": 0.5,  # Was 0.05\n",
        "    \"action_rate\": 0.2,\n",
        "    \"action_sym\": 2.5,\n",
        "    \"joint_velocity\": 0.01*0,\n",
        "    \"joint_acceleration\": 2.5e-7*0,\n",
        "    \"orientation\": 1.0*0,\n",
        "    \"collision\": 1.0*0,\n",
        "    \"default_joint_position\": 0.0\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "def make_env(rank):\n",
        "    def _init():\n",
        "        env = QuadrupedEnv()\n",
        "        env.set_weights(reward_weights, cost_weights)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "def cleanup_env(envs):\n",
        "\n",
        "      env.close()\n",
        "\n",
        "\n",
        "num_envs = 2\n",
        "# env = DummyVecEnv([make_env(i) for i in range(num_envs)])\n",
        "# cleanup_env(env)\n",
        "env = SubprocVecEnv([make_env(i) for i in range(num_envs)])\n",
        "\n",
        "\n",
        "train_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "\n",
        "\n",
        "\n",
        "retrain = False\n",
        "\n",
        "if retrain:\n",
        "    policy_kwargs = {\n",
        "        'net_arch': [\n",
        "            dict(pi=[512,256, 128], vf=[512,256, 128])\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    model = PPO(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=0.001,\n",
        "        n_steps=350,\n",
        "        batch_size=64,\n",
        "        n_epochs=3,\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.92,\n",
        "        clip_range=0.2,\n",
        "        ent_coef=0.01,\n",
        "        policy_kwargs=policy_kwargs,\n",
        "        verbose=0,\n",
        "        tensorboard_log=tensorboard_root_log_directory,\n",
        "    )\n",
        "\n",
        "\n",
        "else:\n",
        "    # name=\"./Models/Mujoco v 3 2025-08-13_08-05-45\"\n",
        "\n",
        "    name =\"./Models/Mujoco v 3 2025-08-17_14-18-45\"\n",
        "\n",
        "    model = PPO.load(name, env=env)\n",
        "    model.ent_coef=0.001\n",
        "    model.learning_rate=0.0001\n",
        "    # model.clip_range=0.1\n",
        "\n",
        "\n",
        "checkpoint_cb = CheckpointCallback(\n",
        "    save_freq=100_000,\n",
        "    save_path=\"./checkpoints/\",\n",
        "    name_prefix=train_time,\n",
        ")\n",
        "\n",
        "log_dir = \"./Monitor/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "debug_callback = RewardCSVLoggerCallback(f\"{log_dir}reward_log{train_time}.csv\",verbose=1,log_freq=10,print_freq=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgLwOd_jrToN",
        "outputId": "3c4e661d-3399-49d9-9694-c6280e45e96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "status: yaw out of range (0.187 not in [-0.17453292519943295, 0.17453292519943295])\n",
            "Episode 300\n",
            "step count: 60716\n",
            "  EpReward |   TrackLin |   TrackAng |    Healthy |    AirTime |     Torque |    Î”Action |  action_sym |    yaw vel\n",
            "     -7.54 |      13.94 |       0.61 |      15.00 |      16.80 |     -27.80 |      -9.17 |     -10.83  |      -6.09 |\n"
          ]
        }
      ],
      "source": [
        "model.learn(\n",
        "    total_timesteps=  1_000_000,\n",
        "    callback=[checkpoint_cb,debug_callback],\n",
        "    # reset_num_timesteps=False if not retrain else True,\n",
        "    tb_log_name=TENSORBOARD_RUN_NAME,\n",
        "    log_interval=100,\n",
        "    progress_bar=False,\n",
        ")\n",
        "cleanup_env(env)\n",
        "train_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "model_dir =  \"./Models/\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "name = f\"{model_dir}Mujoco v 3 {train_time}\"\n",
        "model.save(name)\n",
        "print(f\"Model saved as {name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QovMLCuWm6L"
      },
      "outputs": [],
      "source": [
        "train_time = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "model_dir =  \"./Models/\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "name = f\"{model_dir}Mujoco v 3 {train_time}\"\n",
        "model.save(name)\n",
        "print(f\"Model saved as {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvTJ4j5JdmXP"
      },
      "outputs": [],
      "source": [
        "env = QuadrupedEnv()\n",
        "env.set_weights(reward_weights, cost_weights)\n",
        "obs,info = env.reset()\n",
        "actions = []\n",
        "action1 = []\n",
        "height= []\n",
        "vel_z = []\n",
        "vel_x = []\n",
        "vel_y = []\n",
        "joint_position = []\n",
        "effort = []\n",
        "joint_omega = []\n",
        "prev_action = env._default_joint_position\n",
        "for _ in range(150):\n",
        "    clc()\n",
        "    print(f\"step: {_}\")\n",
        "    action, _ = model.predict(obs)\n",
        "    obs, reward, done,trunc, _ = env.step(action)\n",
        "\n",
        "    effort.append(env.get_joint_effort())\n",
        "    action1.append(action)\n",
        "    ALPHA =0.7\n",
        "    actions.append(ALPHA*prev_action + (1-ALPHA)*action)\n",
        "    joint_position.append(env._get_joint_data())\n",
        "    height.append(env._get_height_data())\n",
        "    vel_x_, vel_y_, vel_z_ = env._get_linear_velocity()\n",
        "    joint_omega.append(env._get_joint_velocity())\n",
        "    # print(env.still_counter)\n",
        "    # print(env.acceleration_cost)\n",
        "    vel_x.append(vel_x_)\n",
        "    vel_y.append(vel_y_)\n",
        "    vel_z.append(vel_z_)\n",
        "    roll,pitch,yaw=env._get_roll_pitch_yaw()\n",
        "    print(roll)\n",
        "    print(pitch)\n",
        "    print(yaw)\n",
        "\n",
        "    prev_action=action\n",
        "    env.render()\n",
        "    # if done or trunc:\n",
        "    #     obs = env.reset()\n",
        "    #     break\n",
        "\n",
        "env.save_animation(\"quadruped_rl.gif\", fps=15)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSlM6GuFdrFg"
      },
      "outputs": [],
      "source": [
        "actions = np.array(actions)\n",
        "action1 = np.array(action1)\n",
        "joint_position = np.array(joint_position)\n",
        "leg_names = ['Front Right', 'Front Left', 'Rear Right', 'Rear Left']\n",
        "joint_indices = {\n",
        "    'Front Right':  [0, 1, 2],\n",
        "    'Front Left': [3, 4, 5],\n",
        "    'Rear Right':   [6, 7, 8],\n",
        "    'Rear Left':  [9, 10, 11],\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "axs = axs.flatten()\n",
        "\n",
        "for i, leg in enumerate(leg_names):\n",
        "    ax = axs[i]\n",
        "    for j, joint_idx in enumerate(joint_indices[leg]):\n",
        "        ax.plot(actions[:, joint_idx], label=f'Joint {j+1}')\n",
        "        ax.plot(joint_position[:, joint_idx], label=f'Joint {j+1}')\n",
        "        ax.plot(action1[:, joint_idx], label=f'Joint {j+1}')\n",
        "        # x limit\n",
        "    ax.set_title(f'{leg} Leg')\n",
        "    ax.set_xlabel('Timestep')\n",
        "    ax.set_ylabel('Action Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq_4MIDc05GH"
      },
      "outputs": [],
      "source": [
        "joints_effor= np.array(effort)\n",
        "leg_names = ['Front Right', 'Front Left', 'Rear Right', 'Rear Left']\n",
        "joint_indices = {\n",
        "    'Front Right':  [ 0, 1, 2],\n",
        "    'Front Left': [3, 4, 5],\n",
        "    'Rear Right':   [6, 7, 8],\n",
        "    'Rear Left':  [9, 10, 11],\n",
        "}\n",
        "\n",
        "# Plotting\n",
        "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
        "axs = axs.flatten()\n",
        "\n",
        "for i, leg in enumerate(leg_names):\n",
        "    ax = axs[i]\n",
        "    for j, joint_idx in enumerate(joint_indices[leg]):\n",
        "        ax.plot(joints_effor[:, joint_idx], label=f'Joint {j+1}')\n",
        "        ax.set_title(f'{leg} Leg')\n",
        "    ax.set_xlabel('Timestep')\n",
        "    ax.set_ylabel('effort Value')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvdZyPzndtwB"
      },
      "outputs": [],
      "source": [
        "# plotting height and vel_z\n",
        "fig , axs = plt.subplots(2,2,figsize=(12,8))\n",
        "axs[0,0].plot(height)\n",
        "axs[0,0].set_title(\"Height\")\n",
        "axs[0,0].set_xlabel(\"Timestep\")\n",
        "axs[0,0].set_ylabel(\"Height\")\n",
        "axs[0,0].grid(True)\n",
        "\n",
        "axs[0,1].plot(vel_z)\n",
        "axs[0,1].set_title(\"Vel_z\")\n",
        "axs[0,1].set_xlabel(\"Timestep\")\n",
        "axs[0,1].set_ylabel(\"Vel_z\")\n",
        "axs[0,1].grid(True)\n",
        "\n",
        "axs[1,0].plot(vel_x)\n",
        "axs[1,0].set_title(\"Vel_x\")\n",
        "axs[1,0].set_xlabel(\"Timestep\")\n",
        "axs[1,0].set_ylabel(\"Vel_x\")\n",
        "axs[1,0].grid(True)\n",
        "\n",
        "axs[1,1].plot(vel_y)\n",
        "axs[1,1].set_title(\"Vel_y\")\n",
        "axs[1,1].set_xlabel(\"Timestep\")\n",
        "axs[1,1].set_ylabel(\"Vel_y\")\n",
        "axs[1,1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaC3FuiqG0bO"
      },
      "outputs": [],
      "source": [
        "import stable_baselines3 as stb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0DaJWKrGpr-"
      },
      "outputs": [],
      "source": [
        "print(f\"numpy version: {np.__version__}\")\n",
        "print(f\"stable-baselines3 version: {stb.__version__}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}